{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check data formatting\n",
    "Once you have compiled a dataset and before you create a fine-tuning job, it is important to check the data formatting. To do this, we created a simple Python script which you can use to find potential errors, review token counts, and estimate the cost of a fine-tuning job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import tiktoken # for token counting\n",
    "import numpy as np\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num examples: 37\n",
      "First example:\n",
      "{'role': 'system', 'content': 'Given an ERC token solidity function as input, return the function annotations in the format accepted by the solc-verify smart contract verifier, just this, nothing more.'}\n",
      "{'role': 'user', 'content': 'function add_to_x(int n) internal;'}\n",
      "{'role': 'assistant', 'content': '/// @notice precondition x == y\\n/// @notice postcondition x == (y + n)\\n/// @notice modifies x'}\n"
     ]
    }
   ],
   "source": [
    "data_path = \"./assets/fine-tuning/annotations.jsonl\"\n",
    "\n",
    "# Load the dataset\n",
    "with open(data_path, 'r', encoding='utf-8') as f:\n",
    "    dataset = [json.loads(line) for line in f]\n",
    "\n",
    "# Initial dataset stats\n",
    "print(\"Num examples:\", len(dataset))\n",
    "print(\"First example:\")\n",
    "for message in dataset[0][\"messages\"]:\n",
    "    print(message)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Format validation\n",
    "We can perform a variety of error checks to validate that each conversation in the dataset adheres to the format expected by the fine-tuning API. Errors are categorized based on their nature for easier debugging.\n",
    "\n",
    "1. Data Type Check: Checks whether each entry in the dataset is a dictionary (dict). Error type: data_type.\n",
    "2. Presence of Message List: Checks if a messages list is present in each entry. Error type: missing_messages_list.\n",
    "3. Message Keys Check: Validates that each message in the messages list contains the keys role and content. Error type: message_missing_key.\n",
    "4. Unrecognized Keys in Messages: Logs if a message has keys other than role, content, weight, function_call, and name. Error type: message_unrecognized_key.\n",
    "5. Role Validation: Ensures the role is one of \"system\", \"user\", or \"assistant\". Error type: unrecognized_role.\n",
    "6. Content Validation: Verifies that content has textual data and is a string. Error type: missing_content.\n",
    "7. Assistant Message Presence: Checks that each conversation has at least one message from the assistant. Error type: example_missing_assistant_message.\n",
    "8. The code below performs these checks, and outputs counts for each type of error found are printed. This is useful for debugging and ensuring the dataset is ready for the next steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found errors:\n",
      "missing_content: 1\n"
     ]
    }
   ],
   "source": [
    "# Format error checks\n",
    "format_errors = defaultdict(int)\n",
    "\n",
    "for ex in dataset:\n",
    "    if not isinstance(ex, dict):\n",
    "        format_errors[\"data_type\"] += 1\n",
    "        continue\n",
    "        \n",
    "    messages = ex.get(\"messages\", None)\n",
    "    if not messages:\n",
    "        format_errors[\"missing_messages_list\"] += 1\n",
    "        continue\n",
    "        \n",
    "    for message in messages:\n",
    "        if \"role\" not in message or \"content\" not in message:\n",
    "            format_errors[\"message_missing_key\"] += 1\n",
    "        \n",
    "        if any(k not in (\"role\", \"content\", \"name\", \"function_call\", \"weight\") for k in message):\n",
    "            format_errors[\"message_unrecognized_key\"] += 1\n",
    "        \n",
    "        if message.get(\"role\", None) not in (\"system\", \"user\", \"assistant\", \"function\"):\n",
    "            format_errors[\"unrecognized_role\"] += 1\n",
    "            \n",
    "        content = message.get(\"content\", None)\n",
    "        function_call = message.get(\"function_call\", None)\n",
    "        \n",
    "        if (not content and not function_call) or not isinstance(content, str):\n",
    "            format_errors[\"missing_content\"] += 1\n",
    "    \n",
    "    if not any(message.get(\"role\", None) == \"assistant\" for message in messages):\n",
    "        format_errors[\"example_missing_assistant_message\"] += 1\n",
    "\n",
    "if format_errors:\n",
    "    print(\"Found errors:\")\n",
    "    for k, v in format_errors.items():\n",
    "        print(f\"{k}: {v}\")\n",
    "else:\n",
    "    print(\"No errors found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upload a training file\n",
    "Once you have the data validated, the file needs to be uploaded using the Files API in order to be used with a fine-tuning jobs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FileObject(id='file-jHmS3gdfrx8QiOHCRhTTGrZt', bytes=12889, created_at=1712811771, filename='erc20_examples.jsonl', object='file', purpose='fine-tune', status='processed', status_details=None)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "api_key = open(\"api_key.txt\").read().strip()\n",
    "\n",
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(api_key=api_key)\n",
    "\n",
    "client.files.create(\n",
    "  file=open(\"erc20_examples.jsonl\", \"rb\"),\n",
    "  purpose=\"fine-tune\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a fine-tuned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FineTuningJob(id='ftjob-Q5UzrRjN624gOCEdQAE2nKBa', created_at=1712811833, error=Error(code=None, message=None, param=None, error=None), fine_tuned_model=None, finished_at=None, hyperparameters=Hyperparameters(n_epochs='auto', batch_size='auto', learning_rate_multiplier='auto'), model='gpt-3.5-turbo-0125', object='fine_tuning.job', organization_id='org-50GaNNrNWMtO9Tsg49g78u9F', result_files=[], status='validating_files', trained_tokens=None, training_file='file-jHmS3gdfrx8QiOHCRhTTGrZt', validation_file=None, user_provided_suffix=None, seed=1167319928, integrations=[])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.fine_tuning.jobs.create(\n",
    "  training_file=\"file-jHmS3gdfrx8QiOHCRhTTGrZt\", \n",
    "  model=\"gpt-3.5-turbo\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
