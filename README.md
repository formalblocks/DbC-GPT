# Extracting Formal Smart-Contract Specifications from Natural Language with LLMs

This repository contains the implementation and experiments for the paper "Extracting formal smart-contract specifications from natural language with LLMs". The project leverages Large Language Models (LLMs), particularly ChatGPT, to automatically infer formal postcondition specifications for smart contract functions implemented in Solidity.

## Introduction

Developers often hesitate to provide formal specifications for software components, even well-established design-by-contract (DbC) properties like invariants, pre- and postconditions are neglected. This project employs state-of-the-art Natural Language (NL) processing technologies using LLMs to automatically infer formal specifications from component textual behavioral descriptions.

We implemented a framework, DbC-GPT, which generates postcondition specifications for smart contract functions implemented in Solidity. The framework uses the solc-verify tool to check the syntax and verify the reference implementation's conformance to the generated specifications.

## Project Structure

```
.
├── .devcontainer
├── .vscode
├── assets
│   ├── file_search
│   ├── fine-tuning
│   └── solc_verify_examples
├── experiments
│   ├── data_analysis
│   ├── loop_files
│   └── outputs
├── solc_verify_generator
│   ├── ERC1155
│   │   ├── imp
│   │   └── templates
│   ├── ERC20
│   │   ├── imp
│   │   │   └── math
│   │   └── templates]
│   ├── ERC721
│   │   ├── imp
│   │   └── templates
│   └── __pycache__
└── temp
```

- `experiments/loop_files`: Folder with scritps to run the iterative specification generation and verification process.
- `experiments/data_analysis`: Folder with scripts for analyzing the results of the verification process.
- `experiments/outputs`: Folder with the results of the experiments in csv format.
- `.env`: Environment variables (to be created by the user).
- `requirements.txt`: List of dependencies.

## Setup

1. **Clone the repository:**
    ```bash
    git clone https://github.com/formalblocks/DbC-GPT.git
    cd DbC-GPT
    ```

2. **Create and activate a virtual environment (optional but recommended):**
    ```bash
    python -m venv venv
    source venv/bin/activate 
    ```

3. **Install dependencies**
    ```bash
    pip install -r requirements.txt
    ```

4. **Set up environment variables:**
Create a .env file in the root directory and add your OpenAI API key.

    ```bash
    OPENAI_API_KEY=your_openai_api_key
    ```

# Smart Contract Verification Analysis

## Overview

This repository contains tools and results for analyzing the verification status of smart contracts. The analysis focuses on examining how well different contract types (ERC20, ERC721, ERC1155) can be formally verified.

## Analysis Results

The verification analysis results are available in the `analysis_results/` directory:

- **Comprehensive Report**: `analysis_results/verification_analysis_report.md`
- **Visualizations**: Various PNG files showing verification success rates and patterns
- **Raw Data**: Processed CSV files with detailed verification results
- **Summary Statistics**: Overview of verification rates by contract and function

### Key Findings

- Overall verification success rate: 60.42%
- ERC1155 contracts have the highest verification success rate (83.33%)
- ERC20 contracts have the lowest verification success rate (50.37%)
- Transfer-related functions are consistently the most challenging to verify
- Functions that involve multiple state changes or complex conditions are harder to verify

## Running the Analysis

The analysis can be run using the Python scripts in the `analysis_results/` directory:

1. **Main Analysis**: `python analysis_results/verification_analysis.py`
2. **Function-Level Analysis**: `python analysis_results/function_analysis.py`

## Data Sources

The analysis is based on verification result data from experimental runs located in:

- `experiments/results_*/` directories

These directories contain CSV files with verification results for different smart contract types and implementations.

## Visualization Types

The analysis includes several types of visualizations:

1. **Bar Charts**: Success rates by contract type and function
2. **Heatmaps**: Success rates across different dimensions
3. **Line Charts**: Progress over experiments
4. **Pie Charts**: Distribution of verification statuses

## Conclusions

The analysis provides valuable insights into smart contract verifiability and highlights areas where improvements can be made to increase the reliability and security of blockchain applications.

For more detailed findings and recommendations, please see the full report at `analysis_results/verification_analysis_report.md`.

# Assistant Performance Comparison Tool

This tool analyzes the performance of different assistants in verifying smart contracts using the results generated by `func_by_func_verifier.py`.

## Requirements

```
pip install pandas matplotlib seaborn
```

## Usage

Simply run the script and it will:
1. Find all result files in the `results_*` directories
2. Analyze the performance metrics
3. Generate visualizations and comparison tables
4. Create a comprehensive report in the `assistant_comparison_report` directory

```bash
python compare_assistants.py
```

## Output

The script generates several visualizations:

1. **Summary Table**: A CSV and HTML table with detailed metrics for each assistant
2. **Verification Rate Bar Chart**: Overall performance of each assistant
3. **Parameter-Specific Plots**: Box plots showing verification rates by:
   - Learning rate
   - Number of epochs
   - Batch size
4. **Heatmaps**: Visualization of verification rates across learning rates and epochs for each batch size
5. **Function-Level Verification**: Heatmap showing which functions are successfully verified by which assistants

## Understanding the Results

- **Verification Rate**: Percentage of runs where the contract was fully verified
- **Average Time**: Average time taken for verification attempts
- **Average Iterations**: Average number of iterations needed
- **Function Verification Rates**: Success rate for each individual function

The hyperparameter analysis (learning rate, epochs, batch size) helps identify the optimal configuration for training.

## Sample Report

A complete report is generated in Markdown format with embedded images at `assistant_comparison_report/report.md`.